{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(folder, start_index=0, num_files_to_process=None):\n",
    "    # List of names of all files in the folder\n",
    "    files = os.listdir(folder)\n",
    "    if num_files_to_process is not None:\n",
    "        files = files[start_index:start_index+num_files_to_process]\n",
    "    num_files = len(files)\n",
    "    print(f\"Number of files in the folder: {num_files}\")\n",
    "\n",
    "    # Dictionary to store the data\n",
    "    data = {}\n",
    "\n",
    "    # Loop over all files\n",
    "    for i, file in enumerate(files, start=1):\n",
    "        print(f\"Processing file {i}/{num_files}...\")\n",
    "        # Open the file\n",
    "        with h5.File(os.path.join(folder, file), 'r') as f:\n",
    "            # Dictionary to store the data for this file\n",
    "            file_data = {}\n",
    "\n",
    "            # Loop over all groups in the file\n",
    "            for group_key in f.keys():\n",
    "                group = f[group_key]\n",
    "\n",
    "                # Dictionary to store the data for this group\n",
    "                group_data = {}\n",
    "\n",
    "                # Check if the group is a dataset or another group\n",
    "                if isinstance(group, h5.Dataset):\n",
    "                    # Read the dataset into a numpy array\n",
    "                    array = np.empty(group.shape, dtype=group.dtype)\n",
    "                    group.read_direct(array)\n",
    "\n",
    "                    # Store the array in the group data dictionary\n",
    "                    group_data[group_key] = array\n",
    "                else:\n",
    "                    # Loop over all subgroups/datasets in the group\n",
    "                    for subgroup_key in group.keys():\n",
    "                        subgroup = group[subgroup_key]\n",
    "\n",
    "                        # Check if the subgroup is a dataset or another group\n",
    "                        if isinstance(subgroup, h5.Dataset):\n",
    "                            # Read the dataset into a numpy array\n",
    "                            array = np.empty(subgroup.shape, dtype=subgroup.dtype)\n",
    "                            subgroup.read_direct(array)\n",
    "\n",
    "                            # Store the array in the group data dictionary\n",
    "                            group_data[subgroup_key] = array\n",
    "                        else:\n",
    "                            # Dictionary to store the data for this subgroup\n",
    "                            subgroup_data = {}\n",
    "\n",
    "                            # Loop over all datasets in the subgroup\n",
    "                            for dataset_key in subgroup.keys():\n",
    "                                dataset = subgroup[dataset_key]\n",
    "\n",
    "                                # Read the dataset into a numpy array\n",
    "                                array = np.empty(dataset.shape, dtype=dataset.dtype)\n",
    "                                dataset.read_direct(array)\n",
    "\n",
    "                                # Store the array in the subgroup data dictionary\n",
    "                                subgroup_data[dataset_key] = array\n",
    "\n",
    "                            # Store the subgroup data in the group data dictionary\n",
    "                            group_data[subgroup_key] = subgroup_data\n",
    "\n",
    "                # Store the group data in the file data dictionary\n",
    "                file_data[group_key] = group_data\n",
    "\n",
    "            # Store the file data in the data dictionary\n",
    "            data[file] = file_data\n",
    "\n",
    "    return data\n",
    "\n",
    "# Load the training data\n",
    "train_folder = 'D:\\\\Datasets\\\\g2net-detecting-continuous-gravitational-waves (1)\\\\train'\n",
    "train_data = load_data(train_folder, start_index=0, num_files_to_process=100)\n",
    "\n",
    "train_labels = 'D:\\\\Datasets\\\\g2net-detecting-continuous-gravitational-waves (1)\\\\train_labels.csv'\n",
    "reading = pd.read_csv(train_labels)\n",
    "print(reading.to_string())\n",
    "\n",
    "# Load the test data\n",
    "test_folder = 'D:\\\\Datasets\\\\g2net-detecting-continuous-gravitational-waves (1)\\\\train'  # Note: We're still using the train folder\n",
    "test_data = load_data(test_folder, start_index=100, num_files_to_process=50)  # Start from the 101st file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in train_data.keys():\n",
    "    print(file)\n",
    "    for group in train_data[file].keys():\n",
    "        print(group)\n",
    "        for subgroup in train_data[file][group].keys():\n",
    "            print(subgroup)\n",
    "            for dataset in train_data[file][group][subgroup].keys():\n",
    "                print(dataset)\n",
    "                print(train_data[file][group][subgroup][dataset].shape)\n",
    "                print(train_data[file][group][subgroup][dataset])\n",
    "                break\n",
    "            break\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py as h5\n",
    "from scipy.signal import stft\n",
    "\n",
    "\n",
    "def preprocess_data(data, hdf5_output_file):\n",
    "    num_files = len(data.keys())\n",
    "    print(\"Length of data:\", str(num_files))\n",
    "    # Loop over all files\n",
    "    for i, file in enumerate(data.keys(), start=1):\n",
    "        print(f\"Processing file {i}/{num_files}...\")\n",
    "        # Loop over all groups in the file\n",
    "        for group in data[file].keys():\n",
    "            # Loop over all subgroups in the group\n",
    "            for subgroup in data[file][group].keys():\n",
    "                if isinstance(data[file][group][subgroup], dict) and 'SFTs' in data[file][group][subgroup]:\n",
    "                    # Get the SFTs\n",
    "                    sfts = data[file][group][subgroup]['SFTs']\n",
    "\n",
    "                    # Apply STFT\n",
    "                    _, _, sfts_stft = stft(sfts)\n",
    "                    sfts_stft = np.abs(sfts_stft)  # Retain the magnitude of the STFT\n",
    "\n",
    "                    # Open the HDF5 file\n",
    "                    with h5.File(hdf5_output_file, 'a') as f:\n",
    "                        # Check if a group for this file already exists in the HDF5 file\n",
    "                        # If not, create it\n",
    "                        file_group_name = f\"{file}_{i}\"\n",
    "                        if file_group_name in f:\n",
    "                            file_group = f[file_group_name]\n",
    "                        else:\n",
    "                            file_group = f.create_group(file_group_name)\n",
    "\n",
    "                        # Check if a subgroup for this group already exists in the file group\n",
    "                        # If not, create it\n",
    "                        if group in file_group:\n",
    "                            group_subgroup = file_group[group]\n",
    "                        else:\n",
    "                            group_subgroup = file_group.create_group(group)\n",
    "\n",
    "                        # Check if a dataset for the STFT of SFTs already exists in the subgroup\n",
    "                        # If not, create it\n",
    "                        if 'SFTs' in group_subgroup:\n",
    "                            sfts_dataset = group_subgroup['SFTs']\n",
    "                        else:\n",
    "                            sfts_dataset = group_subgroup.create_dataset('SFTs', data=sfts_stft)\n",
    "\n",
    "# Preprocess the training data\n",
    "preprocess_data(train_data, 'D:/Datasets/preprocessed_data/preprocessed_data.h5')\n",
    "\n",
    "for file in train_data.keys():\n",
    "    print(file)\n",
    "    for group in train_data[file].keys():\n",
    "        print(group)\n",
    "        for subgroup in train_data[file][group].keys():\n",
    "            print(subgroup)\n",
    "            for dataset in train_data[file][group][subgroup].keys():\n",
    "                print(dataset)\n",
    "                print(train_data[file][group][subgroup][dataset].shape)\n",
    "                print(train_data[file][group][subgroup][dataset])\n",
    "                break\n",
    "            break\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def print_var_sizes():\n",
    "    for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "                             key= lambda x: -x[1])[:2]:\n",
    "        print(\"{:>30}: {:>8}\".format(name, size))\n",
    "\n",
    "print_var_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py as h5\n",
    "\n",
    "def print_structure(hdf5_file):\n",
    "    def print_group(group, indent=\"\"):\n",
    "        print(indent, group.name)\n",
    "        for key in group.keys():\n",
    "            item = group[key]\n",
    "            if isinstance(item, h5.Dataset):  # Dataset\n",
    "                print(indent + \"  \", key)\n",
    "            elif isinstance(item, h5.Group):  # Group\n",
    "                print_group(item, indent + \"  \")\n",
    "    with h5.File(hdf5_file, 'r') as f:\n",
    "        print_group(f)\n",
    "\n",
    "print_structure('D:/Datasets/preprocessed_data/preprocessed_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sft in X:\n",
    "    print(sft.shape, sft.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T11:32:16.100869300Z",
     "start_time": "2023-06-18T11:32:11.753977Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import joblib\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "# Load the labels from the CSV file\n",
    "labels_df = pd.read_csv('D:\\\\Datasets\\\\g2net-detecting-continuous-gravitational-waves (1)\\\\train_labels.csv')\n",
    "labels_df.set_index('id', inplace=True)  # set 'id' as the index to facilitate lookup\n",
    "\n",
    "# Modify the labels: replace -1 with 1\n",
    "labels_df['target'] = labels_df['target'].replace(-1, 1)\n",
    "\n",
    "# Load the preprocessed data from the HDF5 file\n",
    "with h5.File('D:/Datasets/preprocessed_data/preprocessed_data.h5', 'r') as f:\n",
    "    # Keep each SFT data as a 2D array\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for file in f.keys():\n",
    "        file_group = f[file]\n",
    "        for observation in file_group.keys():\n",
    "            obs_group = file_group[observation]\n",
    "            sfts = obs_group['SFTs'][:]\n",
    "            X.append(np.abs(sfts))  # Keep the STFT as 2D array, no flattening\n",
    "            y.append(labels_df.loc[observation, 'target'])\n",
    "\n",
    "    # Find the maximum shape along each dimension\n",
    "    max_shape = np.max([sft.shape for sft in X], axis=0)\n",
    "\n",
    "    # Resize each SFT to the maximum shape\n",
    "    X = [zoom(sft, (max_shape[0] / sft.shape[0], max_shape[1] / sft.shape[1], max_shape[2] / sft.shape[2])) for sft in X]\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "# Here we should apply normalization across each feature independently, not across all values\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, \"scaler.save\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save the training and validation splits to disk\n",
    "with h5.File('D:/Datasets/preprocessed_data/X_train.h5', 'w') as f:\n",
    "    f.create_dataset('X_train', data=X_train)\n",
    "\n",
    "with h5.File('D:/Datasets/preprocessed_data/X_val.h5', 'w') as f:\n",
    "    f.create_dataset('X_val', data=X_val)\n",
    "\n",
    "with h5.File('D:/Datasets/preprocessed_data/y_train.h5', 'w') as f:\n",
    "    f.create_dataset('y_train', data=y_train)\n",
    "\n",
    "with h5.File('D:/Datasets/preprocessed_data/y_val.h5', 'w') as f:\n",
    "    f.create_dataset('y_val', data=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X, scaler, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import h5py as h5\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Conv3D(32, (3, 3, 3), activation='relu', input_shape=(None, None, None, 1)),  # Shape will be determined by the data\n",
    "    MaxPooling3D((2, 2, 2)),\n",
    "    Conv3D(64, (3, 3, 3), activation='relu'),\n",
    "    MaxPooling3D((2, 2, 2)),\n",
    "    tf.keras.layers.GlobalAveragePooling3D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.00001), \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "def generate_batches(X_file, y_file, batch_size):\n",
    "    while True:\n",
    "        with h5.File(X_file, 'r') as X_f, h5.File(y_file, 'r') as y_f:\n",
    "            X_dataset = X_f['X_train']  # or 'X_val', depending on the file\n",
    "            y_dataset = y_f['y_train']  # or 'y_val', depending on the file\n",
    "            num_batches = X_dataset.shape[0] // batch_size\n",
    "\n",
    "            for i in range(num_batches):\n",
    "                start = i * batch_size\n",
    "                end = start + batch_size\n",
    "                X_batch = X_dataset[start:end][..., np.newaxis]\n",
    "                y_batch = y_dataset[start:end]\n",
    "                yield X_batch, y_batch\n",
    "\n",
    "batch_size = 32  # Set batch size\n",
    "train_gen = generate_batches('D:/Datasets/preprocessed_data/X_train.h5', \n",
    "                             'D:/Datasets/preprocessed_data/y_train.h5', \n",
    "                             batch_size)\n",
    "\n",
    "val_gen = generate_batches('D:/Datasets/preprocessed_data/X_val.h5', \n",
    "                           'D:/Datasets/preprocessed_data/y_val.h5', \n",
    "                           batch_size)\n",
    "\n",
    "num_train_samples = 10000  # Update this with your actual number of training samples\n",
    "num_val_samples = 2000  # Update this with your actual number of validation samples\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_gen, \n",
    "                    steps_per_epoch=num_train_samples // batch_size, \n",
    "                    validation_data=val_gen, \n",
    "                    validation_steps=num_val_samples // batch_size, \n",
    "                    epochs=10)\n",
    "\n",
    "# Evaluate the model using the validation generator\n",
    "val_loss, val_accuracy = model.evaluate(val_gen, steps=num_val_samples // batch_size)\n",
    "print(\"Validation Loss: \", val_loss)\n",
    "print(\"Validation Accuracy: \", val_accuracy)\n",
    "\n",
    "# Get the model's predictions on the validation data\n",
    "# Note that this might not be the exact equivalent of model.predict(X_val)\n",
    "# because it might not cover the whole validation set depending on the number of steps\n",
    "probabilities = model.predict(val_gen, steps=num_val_samples // batch_size)\n",
    "\n",
    "# Convert the probabilities into class predictions\n",
    "predictions = (probabilities > 0.5).astype(\"int32\")\n",
    "\n",
    "# Now, here is the tricky part. If you want to compute the classification report, you would need the true labels.\n",
    "# If you can afford to load just the labels into memory, you can do so.\n",
    "# Otherwise, you might need to adjust your generator to yield the IDs as well, and then collect these during prediction.\n",
    "with h5.File('D:/Datasets/preprocessed_data/y_val.h5', 'r') as f:\n",
    "    y_val = f['y_val'][:]\n",
    "\n",
    "print(classification_report(y_val[:len(predictions)], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Instantiate RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Apply RandomOverSampler to the data\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train the model using the resampled data\n",
    "model.fit(X_train_ros, y_train_ros, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "print(\"Validation Loss: \", val_loss)\n",
    "print(\"Validation Accuracy: \", val_accuracy)\n",
    "\n",
    "# Get the model's predictions on the validation data\n",
    "probabilities = model.predict(X_val)\n",
    "\n",
    "# Convert the probabilities into class predictions\n",
    "predictions = (probabilities > 0.5).astype(\"int32\")\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "print(classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Reshape the data to 3D for LSTM  [samples, timesteps, features]\n",
    "X_train_reshaped = X_train_ros.reshape((X_train_ros.shape[0], 1, X_train_ros.shape[1]))\n",
    "X_val_reshaped = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.00001), \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_train_ros, validation_data=(X_val_reshaped, y_val), epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_accuracy = model.evaluate(X_val_reshaped, y_val)\n",
    "print(\"Validation Loss: \", val_loss)\n",
    "print(\"Validation Accuracy: \", val_accuracy)\n",
    "\n",
    "# Get the model's predictions on the validation data\n",
    "probabilities = model.predict(X_val_reshaped)\n",
    "\n",
    "# Convert the probabilities into class predictions\n",
    "predictions = (probabilities > 0.5).astype(\"int32\")\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "print(classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_train_ros, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "def preprocess_data(data, hdf5_output_file):\n",
    "    num_files = len(data.keys())\n",
    "    # Loop over all files\n",
    "    for i, file in enumerate(data.keys(), start=1):\n",
    "        # Loop over all groups in the file\n",
    "        for group in data[file].keys():\n",
    "            # Loop over all subgroups in the group\n",
    "            for subgroup in data[file][group].keys():\n",
    "                if isinstance(data[file][group][subgroup], dict) and 'SFTs' in data[file][group][subgroup]:\n",
    "                    # Get the SFTs\n",
    "                    sfts = data[file][group][subgroup]['SFTs']\n",
    "\n",
    "                    # Apply FFT\n",
    "                    sfts_fft = np.real(np.fft.fft(sfts))\n",
    "\n",
    "                    # Open the HDF5 file\n",
    "                    with h5.File(hdf5_output_file, 'a') as f:\n",
    "                        # Check if a group for this file already exists in the HDF5 file\n",
    "                        # If not, create it\n",
    "                        file_group_name = f\"{file}_{i}\"\n",
    "                        if file_group_name in f:\n",
    "                            file_group = f[file_group_name]\n",
    "                        else:\n",
    "                            file_group = f.create_group(file_group_name)\n",
    "\n",
    "                        # Check if a subgroup for this group already exists in the file group\n",
    "                        # If not, create it\n",
    "                        if group in file_group:\n",
    "                            group_subgroup = file_group[group]\n",
    "                        else:\n",
    "                            group_subgroup = file_group.create_group(group)\n",
    "\n",
    "                        # Check if a dataset for the FFT of SFTs already exists in the subgroup\n",
    "                        # If not, create it\n",
    "                        if 'SFTs' in group_subgroup:\n",
    "                            sfts_dataset = group_subgroup['SFTs']\n",
    "                        else:\n",
    "                            sfts_dataset = group_subgroup.create_dataset('SFTs', data=sfts_fft)\n",
    "\n",
    "# The folder that contains your test data\n",
    "test_folder = 'D:\\\\Datasets\\\\g2net-detecting-continuous-gravitational-waves (1)\\\\test'\n",
    "\n",
    "# The list of test files\n",
    "test_files = os.listdir(test_folder)\n",
    "\n",
    "# Define the output path for preprocessed test data\n",
    "hdf5_output_file = 'D:/Datasets/preprocessed_data/preprocessed_test_data.h5'\n",
    "\n",
    "# Initialize a StandardScaler instance for data normalization\n",
    "scaler = joblib.load(\"scaler.save\")\n",
    "\n",
    "# Create an empty DataFrame to store the predictions\n",
    "predictions_df = pd.DataFrame(columns=['id', 'target'])\n",
    "\n",
    "# Loop over each test file\n",
    "for i, test_file in enumerate(test_files, start=1):\n",
    "    file_start_time = time.time()\n",
    "    print(f\"Processing test file {i}/{len(test_files)}...\")\n",
    "    \n",
    "    # Load the test file\n",
    "    test_data = load_data(test_folder, start_index=i-1, num_files_to_process=1)\n",
    "    \n",
    "    # Preprocess the test data\n",
    "    preprocess_data(test_data, hdf5_output_file)\n",
    "\n",
    "    # Load the preprocessed test data\n",
    "    with h5.File(hdf5_output_file, 'r') as f:\n",
    "        X_test = []\n",
    "        for file in f.keys():\n",
    "            file_group = f[file]\n",
    "            for observation in file_group.keys():\n",
    "                obs_group = file_group[observation]\n",
    "                sfts = obs_group['SFTs'][:]\n",
    "                flattened_sfts = np.real(sfts).flatten()\n",
    "                X_test.append(flattened_sfts)\n",
    "        \n",
    "        max_len = joblib.load(\"max_len.save\")\n",
    "        max_len = max(max_len, max(len(sft) for sft in X_test))\n",
    "        X_test = [np.pad(sft, (0, max_len - len(sft))) for sft in X_test]\n",
    "        X_test = np.array(X_test)\n",
    "        \n",
    "        # Normalize the data\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # Make a prediction\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        binary_predictions = np.round(predictions).astype(int)\n",
    "        \n",
    "        print(\"Prediction for the file: \" + str(binary_predictions))\n",
    "        \n",
    "        # Append the predictions to the DataFrame\n",
    "        for j, prediction in enumerate(binary_predictions):\n",
    "            # Remove the extension from the test file name\n",
    "            file_id = os.path.splitext(test_file)[0]\n",
    "            predictions_df = predictions_df.append({'id': file_id, 'target': prediction}, ignore_index=True)\n",
    "        \n",
    "    # Delete the current test file from the preprocessed_test_data.h5 to save disk space\n",
    "    with h5.File(hdf5_output_file, 'a') as f:\n",
    "        del f[list(f.keys())[0]]  # delete the first key, which is the current test file\n",
    "        \n",
    "    # compute the elapsed time for this file\n",
    "    file_end_time = time.time()\n",
    "    file_elapsed_time = file_end_time - file_start_time\n",
    "\n",
    "    # estimate the remaining time\n",
    "    remaining_files = len(test_files) - i\n",
    "    estimated_time_left = remaining_files * file_elapsed_time\n",
    "    print(f\"Estimated time left: {estimated_time_left} seconds\")\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "predictions_df.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
